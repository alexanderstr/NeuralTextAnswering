#The code uses tensorflow to train a model using a text file from google drive
#and replies to text question (who is the main characted in my case)

#Imports: This section imports necessary libraries and modules. 
#TensorFlow is used for building and training neural network models, 
#NumPy for numerical operations, 
#NLTK for natural language processing tasks like sentence tokenization,
#and OS for interacting with the operating system.

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.tokenize import sent_tokenize
import nltk
import os

#Training Data Path: Specifies the path to the text file that will be used as training data, a book in my case
training_data_path = '/content/drive/My Drive/train.txt'

#Mount Google Drive: This code mounts your Google Drive to the Colab environment, allowing access to files stored there.
from google.colab import drive
drive.mount('/content/drive')

#Download NLTK Resources: Downloads necessary NLTK resources for tokenization and processing.
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('corpus')

#Load Data: Reads the entire content of a text file from the specified path into memory.
def load_data():
    with open(training_data_path, 'r', encoding='utf-8') as f:
      text = f.read()

    return text

#Preprocess Text: Tokenizes the loaded text into sentences using NLTKâ€™s sent_tokenize.
def preprocess_text(text):
    sentences = sent_tokenize(text)
    return sentences

# Parameters
#Purpose: This parameter defines the size of the vocabulary used in the tokenizer.
#Explanation: The tokenizer will only keep the most frequent 10,000 words. All other words will not be directly mapped to a unique integer; instead, they might be replaced by a token representing out-of-vocabulary words (such as the oov_tok described below). This helps in managing memory usage and computational efficiency, particularly when dealing with very large datasets.
vocab_size = 10000

#Purpose: This parameter specifies the dimensionality of the embedding vectors.
#Explanation: Each word in the vocabulary is represented by a vector of 16 elements in the embedding space. Embeddings help in capturing semantic properties of words, where semantically similar words are closer in the embedding space. A higher dimension can capture more nuanced relationships but at the cost of increased computational complexity and risk of overfitting.
embedding_dim = 16

#Purpose: This parameter sets the maximum length of sequences after padding.
#Explanation: All sequences (e.g., sentences) will be either truncated or padded to ensure they have exactly 100 tokens. This uniformity is required for batch processing in neural networks. Sequences longer than 100 tokens will be cut off, and shorter ones will be extended using the specified padding strategy.
max_length = 100

#Purpose: Determines where padding is applied to sequences shorter than max_length.
#Explanation: If padding_type is set to 'post', padding is added at the end of the sequence. This is typically preferred as it keeps the original beginning of the sequence intact, which might contain more critical information than the end, and affects some models less, particularly in attention mechanisms and certain types of RNNs.
padding_type='post'

#Purpose: Specifies where truncation happens for sequences longer than max_length.
#Explanation: Similar to padding, if the trunc_type is 'post', the truncation of tokens happens at the end of the sequence. This means that the beginning part of the sequence is preserved, which is often crucial for understanding the context or meaning, especially in NLP tasks.
trunc_type='post'

#Purpose: Represents the token used for out-of-vocabulary words.
#Explanation: The oov_tok stands for "Out Of Vocabulary" token. When a word not present in the top vocab_size most frequent words (as determined by the tokenizer) is encountered, it is replaced by this token during the tokenization process. This helps in handling unseen words or less frequent words that were not included in the fixed vocabulary.
oov_tok = "<OOV>"

# Execute Data Loading and Preprocessing: Loads the text data and processes it into sentences.
text = load_data()  
sentences = preprocess_text(text)

#Tokenizing sentences
#Tokenizer Setup: Initializes a tokenizer that converts sentences to sequences of integers. The tokenizer is configured to handle only the top vocab_size most frequent words and replace all other words with the <OOV> token.
#Fit Tokenizer and Transform Texts: The tokenizer learns the word index mapping and transforms each sentence into a sequence of integers.
#Padding Sequences: Ensures all sequences have the same length by padding or truncating them.
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(sentences)
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

#Neural network model
#Build Model: Constructs a neural network with an embedding layer followed by a global average pooling layer and several dense layers. The model's architecture is suitable for classification or regression on text data.
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    GlobalAveragePooling1D(),
    Dense(64, activation='relu'),  # Increased from 24 to 64
    Dense(24, activation='relu'),  # Additional layer
    Dense(1, activation='sigmoid')
])

#Compile and Train Model: Compiles the model and performs mock training. The actual labels are set to zeros, which should be replaced with correct labels for real training.
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(padded_sequences, np.zeros(len(padded_sequences)), epochs=10, verbose=1)


#Predict Answer Function: Takes a question, processes it using the same tokenizer, and generates a prediction based on the trained model.
#The prediction involves finding the sentence with the highest cosine similarity to the question's embedding.
def predict_answer(model, tokenizer, sentences, question):
    question_seq = tokenizer.texts_to_sequences([question])
    question_padded = pad_sequences(question_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    question_embedding = model.predict(question_padded)
    sentences_embeddings = model.predict(padded_sequences)

    print("Question Embedding:", question_embedding.flatten()[:10])  # Print part of the embedding for brevity
    print("Sample Sentence Embedding:", sentences_embeddings[0].flatten()[:10])  # Ditto

    dot_products = np.dot(sentences_embeddings, question_embedding.T)
    norms = np.linalg.norm(sentences_embeddings, axis=1) * np.linalg.norm(question_embedding)
    norms = np.where(norms == 0, 1e-8, norms)  # Adjusted to avoid division by zero
    similarities = dot_products.flatten() / norms

    print("Similarities:", similarities[:10])  # Print top 10 similarities for brevity
    most_similar_idx = np.argmax(similarities)

    return sentences[most_similar_idx]

# Example usage
question = "who is Berren"
answer = predict_answer(model, tokenizer, sentences, question)
print("Answer:", answer)



